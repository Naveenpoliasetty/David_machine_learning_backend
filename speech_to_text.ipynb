{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do not forget to install ffmpeg inorder to use pydub\n",
    "brew install ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "from deepgram import DeepgramClient, PrerecordedOptions\n",
    "import os\n",
    "import markdown\n",
    "import pdfkit\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key='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'\n",
    ")\n",
    "\n",
    "# The API key we created in step 3\n",
    "DEEPGRAM_API_KEY = '82578abdf9477547c395bd610e8109a9700789bc'\n",
    "\n",
    "def reset_save_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory)\n",
    "\n",
    "def audio_spilter(audio):\n",
    "    directory = 'audio_directory'\n",
    "    reset_save_directory(directory)\n",
    "    song = AudioSegment.from_mp3(audio)\n",
    "    ten_minutes = 10 * 60 * 1000\n",
    "    audio_length = len(song) / 60000\n",
    "    print(f'Audio length: {math.floor(audio_length)} minutes')      \n",
    "    count = 0\n",
    "    file_paths = []\n",
    "    for x in range(0, len(song), ten_minutes):\n",
    "        count += 1\n",
    "        audio_segment = song[x:x+ten_minutes]\n",
    "        file_path = os.path.join(os.path.abspath(directory), f\"audio_split_{count}.mp3\")\n",
    "        audio_segment.export(file_path, format=\"mp3\")\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "    print('Number of splits:', count)\n",
    "    return file_paths\n",
    "\n",
    "def speech_to_text(audio_path):\n",
    "    paths = audio_spilter(audio_path)\n",
    "    transcript = None\n",
    "    \n",
    "    deepgram = DeepgramClient(DEEPGRAM_API_KEY)\n",
    "    for x in paths:\n",
    "        print(x)\n",
    "        with open(f'{x}', mode='rb') as buffer_data:\n",
    "            payload = { 'buffer': buffer_data }\n",
    "\n",
    "            options = PrerecordedOptions(\n",
    "                smart_format=True, model=\"nova-2\", language=\"en-GB\"\n",
    "            )\n",
    "\n",
    "            response = deepgram.listen.rest.v('1').transcribe_file(payload, options)\n",
    "            if transcript:\n",
    "                print(len(transcript.split(' ')))\n",
    "                transcript = transcript + response.results.channels[0].alternatives[0].transcript\n",
    "            else:\n",
    "                transcript = response.results.channels[0].alternatives[0].transcript\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def notes_gen(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert academic writer tasked with producing highly detailed and structured notes based on a provided text. From the given text, extract the key concepts, topics, and themes, and then elaborate on each topic comprehensively. Structure the notes using clear headings, subheadings, and bullet points for clarity. Ensure that the notes are thorough, insightful, and enriched with additional relevant knowledge beyond the text, showcasing deep understanding. The final output should be written in markdown format and consist of 3000 words, with all explanations logically organized and expanded, going beyond the provided content. Focus on delivering advanced, informative, and well-articulated explanations that stand on their own.\",\n",
    "\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def pdf_gen(audio_path):\n",
    "    text = speech_to_text(audio_path)\n",
    "    notes = notes_gen(text)\n",
    "    html_text = markdown.markdown(notes)\n",
    "    pdfkit.from_string(html_text, \"lstm_notes.pdf\")\n",
    "    print(\"PDF created successfully!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 31 minutes\n",
      "Number of splits: 4\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_2.mp3\n",
      "2059\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_3.mp3\n",
      "4060\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_4.mp3\n",
      "5915\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hw8bjjp5fz4v3pv19qrbxkzt` on tokens per minute (TPM): Limit 5000, Used 0, Requested 7721. Please try again in 32.652s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpdf_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/naveenpoliasetty/Downloads/David2.0/david2.0/LSTM_audio.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 84\u001b[0m, in \u001b[0;36mpdf_gen\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf_gen\u001b[39m(audio_path):\n\u001b[1;32m     83\u001b[0m     text \u001b[38;5;241m=\u001b[39m speech_to_text(audio_path)\n\u001b[0;32m---> 84\u001b[0m     notes \u001b[38;5;241m=\u001b[39m \u001b[43mnotes_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     html_text \u001b[38;5;241m=\u001b[39m markdown\u001b[38;5;241m.\u001b[39mmarkdown(notes)\n\u001b[1;32m     86\u001b[0m     pdfkit\u001b[38;5;241m.\u001b[39mfrom_string(html_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_notes.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 67\u001b[0m, in \u001b[0;36mnotes_gen\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnotes_gen\u001b[39m(text):\n\u001b[0;32m---> 67\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert academic writer tasked with producing highly detailed and structured notes based on a provided text. From the given text, extract the key concepts, topics, and themes, and then elaborate on each topic comprehensively. Structure the notes using clear headings, subheadings, and bullet points for clarity. Ensure that the notes are thorough, insightful, and enriched with additional relevant knowledge beyond the text, showcasing deep understanding. The final output should be written in markdown format and consist of 3000 words, with all explanations logically organized and expanded, going beyond the provided content. Focus on delivering advanced, informative, and well-articulated explanations that stand on their own.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmixtral-8x7b-32768\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/resources/chat/completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:1024\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1023\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:1073\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:1024\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1023\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:1073\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/groq/_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1048\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hw8bjjp5fz4v3pv19qrbxkzt` on tokens per minute (TPM): Limit 5000, Used 0, Requested 7721. Please try again in 32.652s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "pdf_gen('/Users/naveenpoliasetty/Downloads/David2.0/david2.0/LSTM_audio.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 31 minutes\n",
      "Number of splits: 4\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_2.mp3\n",
      "2059\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_3.mp3\n",
      "4058\n",
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_4.mp3\n",
      "5913\n"
     ]
    }
   ],
   "source": [
    "text = sppech_to_text('/Users/naveenpoliasetty/Downloads/David2.0/david2.0/LSTM_audio.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fine. So, with that motivation let us go to the next module, where we will talk about long short term memory and gated recurrent units okay. So, now, all this is fine in terms of okay, I gave you a derivation on the board and say that this is not required, but can I give you a concrete example where RNNs also need to selectively read write and forget right? Only then you will be convinced that this kind of morphing is bad in the case of RNNs. So, I will start with that example. And then once we agree that we need selective read write and forget, how do we convert this into some mathematical equations, right? Because conceptually it is fine, but you have to write some equations so that the R n can do some computations where you have selective read write and forget, right? So, that is what we are going to do over the rest of the lecture. So, first let me start with a concrete example, where you want to predict the sentiment of a review using an RNN. So, this is the RNN structure, we have done this in the past that you have a sentence, one word at a time is your every time step, you will feed this to the RNN, and at the last time step you will make a prediction okay. And as I said the RNN reads the document from left to right, and by the time it reaches the end the information obtained from the first few words is completely lost right. Because it is a long document and you continuously writing to the same cell state. So, I will use the information that you had gained at the previous time step, but ideally we want to do the following. We want to forget the information added by stop words, like a and these do not contribute to the sentiment of it. I can ignore these words and still figure out the sentiment of the document. I want to selectively read the information added by previous sentiment bearing words. So, when I have reached the last time step, I should be able to read everything else which had some sentiments before it and focus on those. Just I want to selectively read from these sentiment bearing words, and also I want to selectively write the new information. So, I have read the word performance now, I want to selectively write it to the memory whether I should write it completely or should I only write parts of it or not that is what I need to decide right. So, that is fair this is a typical example where RNN also when it is reading with long documents, it needs to understand what is the important information in the document that needs to be retained and then selectively read right and forget okay. So, I am spending a lot of time on this analogy because you need to really understand that this is important and this is where RNN suffer right. If you are using them for very very long documents, if you have documents of the size 1,000 words, which is not common, which is not uncommon right? Because Wikipedia pages have much more than that per document. So, it is going to be very hard to encode the entire document using an RNN. Not that it is going to become significantly easier with LSTMs or GRUs, but to certain extent it will become easier okay. Now, the next part is how do we convert this intuition into some mathematical equations right. So, let us look at that. So, in this diagram recall that the blue colored vector is called the state of the RNN, it has a finite size. So, now I will just call it as s t belongs to some RNN, and the state is analogous to the whiteboard and sooner or later it will get overloaded with information and we need to take care of this right. So, now, our wish list is selectively read write and forget okay. So, let us start with that. So, what we want to do is that, and this is the problem definition now, that we have computed the state of the RNN, this this is the blue colored vector although it is not blue, but this is the blue colored vector from the previous diagram, where the state of the RNN was computed. I know what the state is at time step s t minus 1. Now, I want from here to here go from here to here that means, from s t minus 1, I want to compute the new state of the, RNN, right. So, I had something written on the whiteboard, I want to write something new, I want to change the state of the whiteboard and this is the new information that has coming to me, right, x t is the new information at time step t. And while doing this I want to make sure that I use selectively write, read and forget. So, these three operations have to come in somewhere in the between. So, that I am true or faithful to the analogy which I have been giving right that is the question. This is our problem definition now, going from s t minus 1 to s t and introducing these 3 operations along the way okay that is what we are interested in doing. And we will go 1 by 1 we will implement each of these 3 items right. So, we will start with selective right. So, recall that in RNNs this is what happens at every time step t, you take the previous time step, previous l state, you take the current input, do you recognize the operation here? How many of you recognize the operation here? Use your hands okay. So, this is nothing, but the following operation and as usual I have ignored the bias okay, is that fine? So, that is what I am representing it as. But now so, one way of looking at it is that, when I am computing s t, I am reading or I am taking the whole of s t minus 1. So, once I have computed s t minus 1, I am writing this to my whiteboard, and then whole of it would be used to compute the next time step okay. But instead of doing this what I want is, I want to read only selective portions of s t minus 1 or rather I want to write only selective portions of s t minus 1. Once I have computed s t minus 1, I do not want to write the whole of it, because then the whole of it will be used to compute the next cell state, I do not want that. I just want to selectively write some portions of it okay. Now, in the strictest case, since I know that s t minus 1 belongs to R n, it is an n dimensional vector. In a strictest case what I could have done is, I could have used a binary decision, that of all these n entries, I am going to read some entries and ignore the others. So, all the other entries I am going to set to 0 fine, that is the strictest thing that you could have done. Now for any of these strictest things what is the soft solution? So, for binary what's the soft solution binary 0 to 1. So, what's the soft solution for that between 0 to 1, to reach some fraction of each of these dimensions. Right? So, let's try to understand what I'm trying to do here. Okay? So, on the 3rd bullet some of these entries should have gone to 0. Right? Okay? So, instead of doing this what we want to do is, we have this vector which has n entries, this is the cell state at t minus 1. Now, I do not want to write the entire vector onto the final cell state, what I want to do is, I will take some fractions of it say 0.2 of this, 0.3 of this, 0.4 of these and then write only that. Do you see the operation that I am trying to do? Right? I want to take some fractions and write only those to the set. Okay? And as I said this is a softer version of the hard decision, which would have been 0 for this, 1 for this, again 0 for this and so on right. How to do this, why to do this all that is not clear, I am just telling you the intuition how and why will become clear later, is that fine okay. So, we want to be able to take s t minus 1 and write only selective portions of it or pass only selective portions of it to s t. So, whenever we compute s t, we do not want to write the whole of s t minus 1, just want to use selective portions of that. So, what we do is, we introduce something known as a gate okay. And so, this gate is o t minus 1 okay. We take the original cell state s t minus 1, do an element wise product with a gate which is known as the output gate, and then write that product to a new vector which is h t minus 1 okay. So, initially this will look confusing, but it will become clear by the end of this lecture okay. So, is that fine? This is what I am trying to do. Again how to do this is not clear, but this still matches the intuition which I have been trying to build that I want to write only selective portions of the data which I already have, is that fine okay. So, each element of o t minus 1 gets multiplied by the corresponding element of s t minus 1, and it decides what fraction is going to be copied and this o t minus 1 is going to be between 0 to 1. But how do I compute o t minus 1? How does the RNN know what fraction of the cell state to get to the next state? How will it do it? We need to learn something, whenever you want to learn something what do we introduce? Everyone parameter, sorry. What did you guys say? Back propagation, Back propagation will do what? It will work in the air or propagate to what? Whenever you want to do some kind of a learning, I want to learn some function, what do I introduce? Parameter. Parameter right. So, that is what we are going to do. We are now going to introduce a parametric form for o t minus 1 right. And remember this throughout in machine learning whenever you want to learn something always introduce a parametric form of that quantity, and then learn the parameters of that function. Do you get this, how many if you get this statement? Okay. This is what we have been saying day from right from class 2 or class 3 right. Always introduce a parametric function for your input and output and learn the parameters of this function. So, that is exactly what I am going to do, I am going to say that o t minus 1 is actually this function, I am just giving you some time to digest this. So, this is a time step t minus 1. So, it depends on the input at time step t minus 1. It also depends on the output at output means whatever comes out of this right. So, the same operation would have happened at time step t minus 2. So, whatever was the output at that state it will also depend on this. So, just take a while to digest this equation, you will see at least 6 more equations of this form in this lecture. So, if you are comfortable with 1 all of them would be clear. So, try to connect the whole story, I have s t minus 1, I do not want to pass it as on or pass it on as it is.To s t. So, I am computing some intermediate value, where I will only selectively write some portions of s t minus 1. And selectively write in the strictest case which should be binary, but that is not what we are interested in. We introduce fractions, if the fraction has to learn binary let it learn, but we will make it fractional that means, we will make it between 0 to 1, hence the sigmoid function right. Remember in one of these lectures we had said that sigmoids are still used, because in RNNs and LSTMs remember and we had said that sigmoids are bad use tanh or use ReLU, but we had ended with sigmoids are still used in the case of recurrent neural networks and LSTMs. So, this is where they are used okay. How many should get that connection? Okay, good fine. So, we use sigmoids because we want the fraction to be between 0 to 1, and we also want some parameterization right. And this is a particular form that we have chosen. There are various equations possible, various things you could have done here. In fact, there are 10 to 15 different variants of LSTMs, I am covering the most popular one which uses the following equation right. So, it says that this is how you will compute the output gate and that gate will regulate how much of the cell state should be passed on from t minus 1 to the next state okay. Everyone clear with this okay. So, now, if you are clear with this give me an equation for h t minus 1, loudly everyone, s t minus 1, is that fine right? So, this is the equation that we will have. So, we have done selective writing and these parameters are no special they will be learned along with the other parameters of the network okay. So, let us spare some thought on that, you got a certain loss at the output okay. Earlier you just had these parameters w u v, which were the parameters of RNN, which you are adjusting to learn this loss. Now, in addition you also have the flexibility to adjust these parameters. So that, if the loss could improve by selectively writing something, then these parameters should be updated accordingly. Right? Maybe you are being over aggressive and making o t minus 1 to be all ones that means, you are passing everything to the next state right. Now, it has the chance, because they have introduced parameters, if it helps the overall loss, it better make these fractions more appropriate. So, that only selective information is passed to the next state. How many if you get this intuition? Okay. So, that is why any time you introduce parameters you have more flexibility in learning whatever you intend to learn. There is remember one clear difference here right and that is why I said that, while I was giving the analogy I was really setting up things, but here there is one distinction, what is the distinction that is there? Ideally what would I have wanted? Suppose I take the example of the review. Okay? And the review was say the movie was long, but really amazing. Okay? Now, which is the word here which is actually trying to mislead? So, overall sentiment is positive. Right? Everyone agrees with that? But which is the word which is misleading long right that means, I need to do what to that word? Forget that word right. Now ideally, I would have wanted someone telling me retain, retain, retain, forget, retain, retain, retain. I would have a label for each of these words and then I could have a loss function, which tells me whether my gates were actually adhering to this decisions or not. Right? So, remember my gates are learning some distribution o t minus 1, which tells me what fraction to retain. And at this particular time step I would have wanted o t minus 1 to be all 0s. Okay? I would have wanted to forget, but this kind of info not just o t minus 1 this will become more clear when I do all the other gates also. So, what I am trying to say is that you should have had some supervision, which tells you which information to retain and which information to forget, but you do not have the supervision right. No one is telling you these are the important words these are not the important words. So, that is the difference between the whiteboard analogy, there you knew exactly which step is important and which step is not important, here you do not know that. All you know is that you have a final loss function which depends on plus or minus whether the this prediction is close to positive or close to negative, and what is the loss, and that loss is what is being back propagated. But the difference now is that, you have introduced a model which can learn to forget somethings right. Earlier you did not have a model which could learn to write or read or forget selectively. Now, you have introduced a model this is a better modeling choice right. So, the same as we have had arguments that you could do y is equal to w transpose x or you could do y is equal to deep neural network of x right. We are making different modeling choices here and with the hope that one modeling choice is better than the other choice. So, just as RNN was one modeling choice, now you are using a different modeling choice where again with the help of these gates and all you can definitely write a function of y is a function of the input and that function is going to be the LSTM function, which we will see in detail. So, this is one part of that function. And while doing this you are just making a better modeling choice which allows you to learn more parameters and along the way if important do selective write read and forget, is that clear right? So, you would see the difference what would have been the ideal case and what is it that you have? The ideal case would have been explicit supervision for what to forget read and write, you will never have that, but you are still making a modeling choice which allows you to do that. So, if it is required the model while back propagation should be able to learn these parameters. So, get you are able to do that. I know I am repeating myself, but it is very important that you understand this distinction. How many of you get this now? And as I said these parameters will be learned along with the other parameters and o t is called the output gate, because it decides what to output to the next cell state okay. Still you see that there is a lot of gap here, we have not reached s t yet, we are still at s t minus 1, we have computed some intermediate value, but we have not reached s t yet. And along the way we had 3 things selective write read and forget, we have only taken care of selective write so far okay. Now, let us look at selective read. So, what does selective read do? You are going to get new information at time step t which is x t right. And now instead of this original cell state, you have used the selectively written cell state, because that is what you have written now. So, that is what you should use. Now using a combination of these 2, I am going to compute some intermediate value okay. And just stare at this equation, this equation form is very similar to the RNN equation form right, only thing is that instead of s t minus 1, I am using h t minus 1 and for good reason, because I know that h t minus 1 contains only selectively written values from s t minus 1, is that fine? And x t is the new input. Still there is some gap here, I have not reached s t yet, I am still at an intermediate value. So, this is the new input which I have received. Now, what should I do with this new input? Selectively read this input, I do not want to take all of this input, because maybe the input which I have got now is a stop word, and I do not want to read all of it right. Do you get that? So, now, it captures all the information from the previous state as well as the current input, and you want to selectively read this. So, now, what would you do to selectively read? Again the same situation that you have an s tilde, the answer is already here. You have an s tilde, and you do not want to pass it on as it is to s t, this is s tilde, s t somewhere here which you do not know how to get to, but you know that you do not want to pass on all the input that you have read, you want to selectively pass it on. So, what will you do now? Again introduce a gate, and this gate will be called? Input gate or the read gate right okay. So now, what can you give me an equation for the gate? I t is equal to? Sigma of okay that is good, because sigmoid is what we will need it is going to be a fractional thing. Let me add the easy part, w into h t minus 1 that is telling you what has happened so far and u times x t. You see the same equation, same form the parameters have changed. So, these you will call as w I u I and b I and they are depending on the input as well as the previous state, previous temporary state that we had computed okay. So, that is exactly what your input gate is going to be and now this operation is the selectively reading operation. How many if you are fine at this point? Okay. And then this product is going to use to be is will help us to read selectively from this temporary value that we have constructed or the input that we have taken okay. So, so far what do we have? We have the following, we have the previous state which was s t minus 1, then we have an output gate which was o t minus 1, using these 2 we have done selective right right. We have taken the previous state and the gate and then a selective right, is that fine okay. And we need to check if the sigmoid should come here, because sigmoid is already there in the computation of s t minus 1 right. It is not there. So, this already has 1 sigmoid right, yeah. So, then again a sigmoid on that is it there, okay. We will figure it out just check the equation right. So, there may or may not be the sigmoid, the sigmoid might already applied to s t minus 1, but we can figure that out okay. So, this is the selective right portion, Then you compute the current temporary state okay, and just look at the similarity between these equations. Then you have an input gate, and using these 2 you have done a selective read okay. So, I have taken care of selective write and selective read, but you are still not reached s t, I still do not have an arrow here, I still need to figure out how to compute the s t finally, okay. So, what is the operation which is remaining now selective forget.Okay. So, what do you think should we forget? Do you want to find new s t? So, let us see what we will forget right. So, the question now is that, you had this s t minus 1, and now you have a temporary state s tilde t, which is here, how do we combine these 2 to get the new cell state okay. So, the simplest way would be, that you say that s t is equal to whatever was there in s t minus 1, plus selectively reading from the current input, is that fine? This is one way of doing it okay. But now what am I doing here, what is the problem here? I am reading, I am taking s t minus 1 as it is right. So, what should I do? I should forget some parts of s t minus 1. So, what should I do for that? Introduce a what gate? Forget gate right. So, we may not want to use s t minus 1 as it is, but we want to forget. So, there is at this point all of you should get some confusion, if you do not then I would be worried, if you are getting some confusion good right, you should all get confused at this point. Why are you confused? Because you already read selective right, and now again you are doing a selective forget also right right. But there is a difference, because the selective right was then used to compute how to read the information right. But now once you have read the new information, you want to see how to assimilate it back with the old information that you had right. So, that is why you introduce a separate gate. So, think of it as this way that, you start keeping these functions separate input, output and forget. So, they can separately learn things okay. So, whatever you want to selective right, let it be a separate function. These h t minus 1 is not going back to s t right. Let us just be used so that you can compute these temporary states. So, that is what is being passed to the next temporary state. Let I t only decide how much of this input should be read okay. And then when you want to combine these 2, just use a separate gate. And this exact idea which is confusing all of you, why have a separate right gate and a separate forget gate, led to something known as gated recurrent units, where they merged these 2 gates okay. So, we will get back to that okay. So, at this point it is fine, I am just telling you the original equations for LSTM and this was the motivation that they had. So, as I said there are at least 15 to 20 different variants of LSTM, which use different equations they tie some of these weights. So, one thing could be that forget is the same as 1 minuteus remember right or output could be same as 1 minuteus input right. So, you could have tied these gates instead of learning separate parameters for them. So, in the most parameterized form you have a separate parameter for all of these okay. So, we introduce the forget gate, again can you tell me a form for this forget gate, f t is equal to first term, w f, second term u f, what would be there in the second term? X t and the first term okay. So, this is what it will look like okay. So, if you remember one of these equations you will be able to write all of these, not that I am going to ask you to write them in a quiz or something, but why take a chance. So, and then once you have computed the forget gate, instead of this equation can you tell me what is the equation that you are going to use? What is the first time going to be? It is s t minus 1 here, what is it going to be now? F t into? S t minus 1, is it fine, okay. So now, we have the full set of equations for LSTM, we have certain gates and certain states. What are the gates? Output gate, input gate, forget gate. Why do you guys have this momentary amnesia like suddenly forget everything? Okay. So, output gate, input gate and forget gate, all of these have the same form with different parameters okay. What about the states, which are the states that we have computed one was s t, the other was h t and the third one was s tilde t okay, s tilde t, from s tilde t we get s t and from s t we compute h t okay. So, yeah. So, in the diagram that you see here at the top, tell me which are the computations which are happening at one time step? At time step t, which are the computations which are happening, is it I will give you the options right, is it this or is it this okay, let us call this 1, let us call this 2 or this 3 or this 4, which are the computations happening at one time step? And you see the order also here, this should be straightforward right, why? How many of you say 4? That is the one right, because you start with selective reading right, and you can just go by these right, these are all index by t right, is that fine okay. So, these are the computations would happen at time step t, and these are exactly the computations which were written right. So, we have the 3 gates, which you need to compute at every time step, and you have the 3 states which you need to compute at every time step, is that fine. And this s t minus 1 is not being computed it is just taken from the previous time step, is that okay fine. So, you have these 6 computations would happen at every time step, and the output final output of an LSTM. So, when you use tensorflow or something, the output of an LSTM would give you two things it will give you h t comma s t okay. Well these are both the states that are being computed, one is the running state and one is the current state which is being computed okay. And I chose the notation s because that is what we have been using for RNNs, but in LSTM and all the literature instead of s you will find it to be c t okay, because it is called the cell state so that is why c t okay. So, all these equations wherever you see an s s, when you are reading some standard blogs or things like that you will see c instead of s right, you just do this mapping in your head okay. So, LSTM actually has many variants, which include different number of gates and also different arrangement of the gates. So, as I was saying that you could say that input is 1 minus output or input is 1 minus forget or things like that. And also why this particular parametric form. Right? Why not make w naught into s t minus 1 instead of h t minus 1 and so on. So, all kinds of things that you could do and all of these are valid, these are all valid variance of LSTMs. So, there is this paper called LSTM or search space ODC. So, you can go and look at I think we link it in the pre in the reading material right. So, you can see that there are actually many, many variants of LSTMs, but this is the more standard and default variant which will find in most platforms on TensorFlow or PyTorch and so on. And there is another very popular variant of LSTMs which is called gated recurrent units. So, we will just see gated recurrent units. So, I will just give you the full set of equations for GRUs. So, you have gates, but unlike LSTMs you have only 2 gates, you have an output gate and you have an input gate, you do not have the forget gate okay. So, what am I going to do for the forget gate? So, this is what I am going to do, you see the last equation. So, instead of forget gate, I am just saying that okay, this is what you are going to selectively input from the current temporary state. So, the rest of you rest of it you take from the previous state right. So, I have just tied the input gate and the forget gate okay. Any other changes do you see in this? So, earlier we had h t minus 1 everywhere right. Now, we have s t minus 1 itself is that fine okay. So, the basic idea right these equations are many many and you could think of your own equations, you could say that I will not really use this input information at all or I will choose to use it differently or what not right. There are several things that you could do, at a very abstract level this is what you need to understand what is this here. So, these parameters could then make a difference right, they could adjust it accordingly and so on right. So, that is what I was coming. So, the there are various ways of realizing this right, at the abstract level you need to understand that the original problem was trying to store all the information from time step 1 to time step t, capital T right, which is not feasible because of this finite size that you have. So, along the way we build this intuition that it should be good to have these operations which allow you to selectively read, write and forget right. How do you mathematically realize these operations? There are various, various choices for doing that, and we saw a few choices for doing that right. There are many others you could have done, but this is largely what whenever you say that I am using LSTM, most likely you are using the set of equations which I saw, which we saw on the previous slide and whenever you are using a GRU these are the set of equations that you will be using okay. And again remember this that there is no explicit supervision here, it is just that we have a better modeling choice, we are just introduce more parameters. So, that if required these parameters could be adjusted to do a selectively read write and forget right. So, it is often it is often valuable, if you are doing some task with RNNs or LSTMs, you should visualize these gates right, you should see that a time step t, if you thought that it should have forgotten everything that it hasSo far, because suppose you had this the movie was long, but I really loved it, because the direction was superb and so on. Now, this word but actually changes everything right, because it whatever was written before it does not matter anymore right. So, is it really learning those kind of gates where everything before, but was forgotten right. So, it would be helpful to visualize these output gates and see what kind of values they are learning, what what kind of things they are remembering, forgettable, forgetting and selectively reading and so on right. So, as I said I will just again summarize the key thing here is the intuition and then the realization in the form of equations there are multiple choices, we have seen a few of those, right that is what I will end with. And in particular in GRUs there is no explicit forget gate and instead of s t minus 1 you use s t minus 1 everywhere.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key='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'\n",
    ")\n",
    "def notes_gen(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"From the given text identify the key topics and concepts and explain them in detail . Organize the notes in a logical flow that reflects the structure of the lecture. Use bullet points, headings, and subheadings where appropriate. The final notes should be in markdown format, please do use headings, subheadings and bullets. Please generate well detailed notes in 3000 words\",\n",
    "\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gemma-7b-it\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF created successfully!\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "import pdfkit\n",
    "\n",
    "notes = notes_gen(text)\n",
    "html_text = markdown.markdown(notes)\n",
    "    \n",
    "# Save HTML to PDF\n",
    "pdfkit.from_string(html_text, \"lstm_notes.pdf\")\n",
    "\n",
    "print(\"PDF created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So, so with that motivation let's go to the next module where we'll talk about long, short term memory and gated recurrent units. So now all this was fine in terms of okay, I gave you a derivation on the board and say that this is not required but can I give you a concrete example where RNNs also need to selectively read, right and forget, only then you will be convinced that this kind of morphing is bad in the case of Iran. So, I'll start with that example. And then once we agree that we need selective read, write and forget, how do we convert this into some mathematical equations, right? Because conceptually it's fine, but you have to write some equations so that the RN can do some computations where you have selective read, right and forget. So, that's what we are going to do over the rest of the lecture. So, first let me start with a concrete example where you want to predict. the sentiment of a review using an RNN. So, this is the RNN structure. We have done this in the past that you have a sentence, one word at a time is your every time step. You will feed this through the RNN and at the last time step you will make a prediction. Okay? And as I said the RN reads a document from left to right and by the time it reaches the end, the information obtained from the first few words is completely lost, right? Because it's a long document and you'll continuously writing to the same cell state. So you'll lose the information that you had gained at the previous time step. But ideally we want to do the following. We want to forget the information added by stop words like A and the these do not contribute to the sentiment of the. I can ignore these words and still figure out the sentiment of the document. I want to selectively read the information added by previous sentiment bearing words. So when I have reached the last time step I should be able to read everything else which had some sentiments before it and focus on those. Just I want to selectively read from these sentiment bearing words and also I want to selectively write the new information. So, I have read the word performance now. I want to selectively write it to the memory. Whether I should write it completely or should I only write parts of it or not, that is what I need to decide. So, that is fair. This is a typical example where RNN also when it's really with long documents, it needs to understand what is the important information in the document that needs to be retained and then selectively read, write and forget So I am spending a lot of time on this analogy because you need to really understand that this is important and this is where RNNN suffer If you are using them for you are using them for very very long documents if we have documents of the size 1 words which is not common which is not uncommon right because Wikipedia pages have much more than that per document. So it's going to be very hard to encode the entire document using an RNNN. Not that it's going to become significantly easier with LSTMs or GRIUs but to certain extent it will become easier. Now the next part is how do we convert this intuition into some mathematical equations right? So let's look at that. So, in this diagram, recall that the blue colored vector is called the state of the RNN. It has a finite size and now I will just call it as ST belongs to some RN and the state is analogous to the whiteboard and sooner or later it will get overloaded with information and we need to take care of this right. So, now our wish list is selectively read, write and forget. Okay, so let's start with that. So, what we want to do is that and this is the problem definition now that we have computed the state of the RNNN This is a blue colored vector although it's not blue but this is the blue colored vector from the previous diagram where the state of the RNN was computed. I know what the state is at time step ST minus 1. Now I want from here to here go from here to here that means from ST minus 1 I want to compute the new state of the RNN. So, I had something written on the white board. I want to write something new, I want to change the state of the white board and this is the new information that has coming to me right. XT is the new information at time step t and while doing this. I want to make sure that I use selectively write, read and forget. So, these three operations have to come in somewhere in the between so that I am true or faithful to the analogy which I have been giving. That is the question. This is our problem definition now going from ST minus 1 to ST and introducing these three operations along the way. That's what we are interested in doing. And we'll go one by one, we'll implement each of these three items. So we'll start with selective. right. So, recall that in RNNs, this is what happens at every time step T. You take the previous time step, previous L state, you take the current input. Do you recognize the operation here? How many if you recognize the operation here? Here is your hands, okay? So, this is nothing but the following operation and as usual I have ignored the bias. Is that fine? So that's what I am representing it as. But now, so. So one way of looking at it is that when I am computing ST I am reading or I am taking the whole of ST minus 1 So once I have computed ST minus 1 I am writing this to my whiteboard and then whole of it would be used to compute the next time step. But instead of doing this, what I want is I want to read only selective portions of ST minus 1 or rather I want to write only selective portions of ST minus 1. Once I have computed ST minus 1, I don't want to write the whole of it because then the whole whole of it will be used to compute the next cell state. I don't want that. I just want to selectively write some portions of it. Okay. Now in the strictest case since I know that ST minus 1 belongs to RN, it's an n dimensional vector. In a strictest case what I could have done is I could have used a binary decision that of all these N entries I'm going to read some entries and ignore the others. So, all the other entries I'm going to set to zero. Fine? That's the strictest thing that you could have done. Now for any of of these strictest things what is the soft solution? So, for binary what is the soft solution? Binary is 0 to 1. So, what is the soft solution for that? Between 0 to 1. So, read some fraction of each of these dimensions. So let's try to understand what I am trying to do here. So, on the third bullet, some of these entries should have gone to 0. Right? So instead of doing this, what we want to do is we have this vector which has n entries. this is the cell state at t minus 1. Now, I don't want to write the entire vector onto the final cell state. What I want to do is I will take some fractions of it, say 0.2 of this, 0.3 of this, 0.4 of these and then write only that. Do you see the operation that I'm trying to do? I want to take some fractions and write only those to the set. And as I said, this is a softer version of the hard decision which would have been, 0 for this, 1 for this, again 0 for this and so on. How to do this, why to do this, all that is not clear. I'm just telling you the intuition how and why will become clear later. Is that fine? Okay. So, we want to be able to take ST minus 1 and write only selective portions of it or pass only selective portions of it to ST. So, whenever we compute ST, we don't want to write the whole of ST minus 1, just want to use selective portions of that. So, what we do is we introduce something. known as a gate and so this gate is OT minus 1 We take the original cell state ST minus 1 do an element wise product with a gate which is known as the output gate and then write that product to a new vector which is ht minus 1. Okay. So, initially this will look confusing but it will become clear by the end of this lecture. Okay. So, is that fine? This is what I am trying to do. Again, how to do this is not clear, but this still matches the intuition which I have been trying to build that I want to write only selective portions of the data which I already have. Is that fine? Okay? So, each element of OT minus 1 gets multiplied by the corresponding element of ST minus 1 and it decides what fraction is going to be copied and this OT minus 1 is going to be between 0 to 1. But how do I compute OT minus 1? How does the RNN in know what fraction of the cell state to get to the next state? How will it do it? We need to learn something. Whenever you want to learn something, what do we introduce? Everyone? Parameter. Sorry. What did you guys say? Back propagation. Back propagation will do what? It will work in the air or propagate to what? Whenever you want to do some kind of a learning, I want to learn some function. What do I introduce? Parameter, right? So, that's what we are going to do. We are now going to introduce a parametric form for OT minus 1. And remember this throughout in machine learning, whenever you want to learn something, always introduce a parametric form of that quantity and then learn the parameters of that function. You get this, how many if you get this statement? Okay, this is what we have been saying they from right from class 2 or class 3, right? Always introduce a parametric function for your input and output and learn the parameters of this function. So, that's exactly what I'm going to do. I'm going to say that OT minus 1 is actually this function. I'm just giving you some time to digest this. So, this is at time step t minus 1. So, it depends on the input at time step t minus 1. It also depends on the output at output means whatever comes out of this right. So, the same operation would have happened at time step t minus 2. So, whatever was the output at that state, it will also depend on this. So, just take a while to digest this equation. You will see at least six more equations of this form in this lecture. So if you are comfortable with 1, all of them would be clear. So, try to connect the whole story. I have ST minus 1, I don't want to pass it as on, pass it on as it is.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq(api_key='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')\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename = '/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3'# Replace with your audio file!\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()), # Required audio file\n",
    "      model=\"distil-whisper-large-v3-en\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",\n",
    "      language=\"en\",\n",
    "      temperature=0.0 \n",
    "    )\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepgram import DeepgramClient, PrerecordedOptions\n",
    "import os\n",
    "\n",
    "# The API key we created in step 3\n",
    "DEEPGRAM_API_KEY = '82578abdf9477547c395bd610e8109a9700789bc'\n",
    "\n",
    "def sppech_tp_text(directory):\n",
    "    paths = os.listdir(directory)\n",
    "    transcript = None\n",
    "    deepgram = DeepgramClient(DEEPGRAM_API_KEY)\n",
    "    for x in paths:\n",
    "        with open(f'{directory}/{x}', 'rb') as buffer_data:\n",
    "            payload = { 'buffer': buffer_data }\n",
    "\n",
    "            options = PrerecordedOptions(\n",
    "                smart_format=True, model=\"nova-2\", language=\"en-GB\"\n",
    "            )\n",
    "\n",
    "            response = deepgram.listen.rest.v('1').transcribe_file(payload, options)\n",
    "            if transcript:\n",
    "                print(len(transcript.split(' ')))\n",
    "                transcript = transcript + response.results.channels[0].alternatives[0].transcript\n",
    "            else:\n",
    "                transcript = response.results.channels[0].alternatives[0].transcript\n",
    "\n",
    "    return transcript\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2059\n",
      "4059\n",
      "5914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Fine. So, with that motivation let us go to the next module, where we will talk about long short term memory and gated recurrent units okay. So, now, all this is fine in terms of okay, I gave you a derivation on the board and say that this is not required, but can I give you a concrete example where RNNs also need to selectively read write and forget right? Only then you will be convinced that this kind of morphing is bad in the case of RNNs. So, I will start with that example. And then once we agree that we need selective read write and forget, how do we convert this into some mathematical equations, right? Because conceptually it is fine, but you have to write some equations so that the R n can do some computations where you have selective read write and forget, right? So, that is what we are going to do over the rest of the lecture. So, first let me start with a concrete example, where you want to predict the sentiment of a review using an RNN. So, this is the RNN structure, we have done this in the past that you have a sentence, one word at a time is your every time step, you will feed this to the RNN, and at the last time step you will make a prediction okay. And as I said the RNN reads the document from left to right, and by the time it reaches the end the information obtained from the first few words is completely lost right. Because it is a long document and you continuously writing to the same cell state. So, I will use the information that you had gained at the previous time step, but ideally we want to do the following. We want to forget the information added by stop words, like a and these do not contribute to the sentiment of it. I can ignore these words and still figure out the sentiment of the document. I want to selectively read the information added by previous sentiment bearing words. So, when I have reached the last time step, I should be able to read everything else which had some sentiments before it and focus on those. Just I want to selectively read from these sentiment bearing words, and also I want to selectively write the new information. So, I have read the word performance now, I want to selectively write it to the memory whether I should write it completely or should I only write parts of it or not that is what I need to decide right. So, that is fair this is a typical example where RNN also when it is reading with long documents, it needs to understand what is the important information in the document that needs to be retained and then selectively read right and forget okay. So, I am spending a lot of time on this analogy because you need to really understand that this is important and this is where RNN suffer right. If you are using them for very very long documents, if you have documents of the size 1,000 words, which is not common, which is not uncommon right? Because Wikipedia pages have much more than that per document. So, it is going to be very hard to encode the entire document using an RNN. Not that it is going to become significantly easier with LSTMs or GRUs, but to certain extent it will become easier okay. Now, the next part is how do we convert this intuition into some mathematical equations right. So, let us look at that. So, in this diagram recall that the blue colored vector is called the state of the RNN, it has a finite size. So, now I will just call it as s t belongs to some RNN, and the state is analogous to the whiteboard and sooner or later it will get overloaded with information and we need to take care of this right. So, now, our wish list is selectively read write and forget okay. So, let us start with that. So, what we want to do is that, and this is the problem definition now, that we have computed the state of the RNN, this this is the blue colored vector although it is not blue, but this is the blue colored vector from the previous diagram, where the state of the RNN was computed. I know what the state is at time step s t minus 1. Now, I want from here to here go from here to here that means, from s t minus 1, I want to compute the new state of the, RNN, right. So, I had something written on the whiteboard, I want to write something new, I want to change the state of the whiteboard and this is the new information that has coming to me, right, x t is the new information at time step t. And while doing this I want to make sure that I use selectively write, read and forget. So, these three operations have to come in somewhere in the between. So, that I am true or faithful to the analogy which I have been giving right that is the question. This is our problem definition now, going from s t minus 1 to s t and introducing these 3 operations along the way okay that is what we are interested in doing. And we will go 1 by 1 we will implement each of these 3 items right. So, we will start with selective right. So, recall that in RNNs this is what happens at every time step t, you take the previous time step, previous l state, you take the current input, do you recognize the operation here? How many of you recognize the operation here? Use your hands okay. So, this is nothing, but the following operation and as usual I have ignored the bias okay, is that fine? So, that is what I am representing it as. But now so, one way of looking at it is that, when I am computing s t, I am reading or I am taking the whole of s t minus 1. So, once I have computed s t minus 1, I am writing this to my whiteboard, and then whole of it would be used to compute the next time step okay. But instead of doing this what I want is, I want to read only selective portions of s t minus 1 or rather I want to write only selective portions of s t minus 1. Once I have computed s t minus 1, I do not want to write the whole of it, because then the whole of it will be used to compute the next cell state, I do not want that. I just want to selectively write some portions of it okay. Now, in the strictest case, since I know that s t minus 1 belongs to R n, it is an n dimensional vector. In a strictest case what I could have done is, I could have used a binary decision, that of all these n entries, I am going to read some entries and ignore the others. So, all the other entries I am going to set to 0 fine, that is the strictest thing that you could have done. Now for any of these strictest things what is the soft solution? So, for binary what's the soft solution binary 0 to 1. So, what's the soft solution for that between 0 to 1, to reach some fraction of each of these dimensions. Right? So, let's try to understand what I'm trying to do here. Okay? So, on the 3rd bullet some of these entries should have gone to 0. Right? Okay? So, instead of doing this what we want to do is, we have this vector which has n entries, this is the cell state at t minus 1. Now, I do not want to write the entire vector onto the final cell state, what I want to do is, I will take some fractions of it say 0.2 of this, 0.3 of this, 0.4 of these and then write only that. Do you see the operation that I am trying to do? Right? I want to take some fractions and write only those to the set. Okay? And as I said this is a softer version of the hard decision, which would have been 0 for this, 1 for this, again 0 for this and so on right. How to do this, why to do this all that is not clear, I am just telling you the intuition how and why will become clear later, is that fine okay. So, we want to be able to take s t minus 1 and write only selective portions of it or pass only selective portions of it to s t. So, whenever we compute s t, we do not want to write the whole of s t minus 1, just want to use selective portions of that. So, what we do is, we introduce something known as a gate okay. And so, this gate is o t minus 1 okay. We take the original cell state s t minus 1, do an element wise product with a gate which is known as the output gate, and then write that product to a new vector which is h t minus 1 okay. So, initially this will look confusing, but it will become clear by the end of this lecture okay. So, is that fine? This is what I am trying to do. Again how to do this is not clear, but this still matches the intuition which I have been trying to build that I want to write only selective portions of the data which I already have, is that fine okay. So, each element of o t minus 1 gets multiplied by the corresponding element of s t minus 1, and it decides what fraction is going to be copied and this o t minus 1 is going to be between 0 to 1. But how do I compute o t minus 1? How does the RNN know what fraction of the cell state to get to the next state? How will it do it? We need to learn something, whenever you want to learn something what do we introduce? Everyone parameter, sorry. What did you guys say? Back propagation, Back propagation will do what? It will work in the air or propagate to what? Whenever you want to do some kind of a learning, I want to learn some function, what do I introduce? Parameter. Parameter right. So, that is what we are going to do. We are now going to introduce a parametric form for o t minus 1 right. And remember this throughout in machine learning whenever you want to learn something always introduce a parametric form of that quantity, and then learn the parameters of that function. Do you get this, how many if you get this statement? Okay. This is what we have been saying day from right from class 2 or class 3 right. Always introduce a parametric function for your input and output and learn the parameters of this function. So, that is exactly what I am going to do, I am going to say that o t minus 1 is actually this function, I am just giving you some time to digest this. So, this is a time step t minus 1. So, it depends on the input at time step t minus 1. It also depends on the output at output means whatever comes out of this right. So, the same operation would have happened at time step t minus 2. So, whatever was the output at that state it will also depend on this. So, just take a while to digest this equation, you will see at least 6 more equations of this form in this lecture. So, if you are comfortable with 1 all of them would be clear. So, try to connect the whole story, I have s t minus 1, I do not want to pass it as on or pass it on as it is.To s t. So, I am computing some intermediate value, where I will only selectively write some portions of s t minus 1. And selectively write in the strictest case which should be binary, but that is not what we are interested in. We introduce fractions, if the fraction has to learn binary let it learn, but we will make it fractional that means, we will make it between 0 to 1, hence the sigmoid function right. Remember in one of these lectures we had said that sigmoids are still used, because in RNNs and LSTMs remember and we had said that sigmoids are bad use tanh or use ReLU, but we had ended with sigmoids are still used in the case of recurrent neural networks and LSTMs. So, this is where they are used okay. How many should get that connection? Okay, good fine. So, we use sigmoids because we want the fraction to be between 0 to 1, and we also want some parameterization right. And this is a particular form that we have chosen. There are various equations possible, various things you could have done here. In fact, there are 10 to 15 different variants of LSTMs, I am covering the most popular one which uses the following equation right. So, it says that this is how you will compute the output gate and that gate will regulate how much of the cell state should be passed on from t minus 1 to the next state okay. Everyone clear with this okay. So, now, if you are clear with this give me an equation for h t minus 1, loudly everyone, s t minus 1, is that fine right? So, this is the equation that we will have. So, we have done selective writing and these parameters are no special they will be learned along with the other parameters of the network okay. So, let us spare some thought on that, you got a certain loss at the output okay. Earlier you just had these parameters w u v, which were the parameters of RNN, which you are adjusting to learn this loss. Now, in addition you also have the flexibility to adjust these parameters. So that, if the loss could improve by selectively writing something, then these parameters should be updated accordingly right. Maybe you are being over aggressive and making o t minus 1 to be all ones that means, you are passing everything to the next state right. Now, it has the chance, because they have introduced parameters, if it helps the overall loss, it better make these fractions more appropriate. So, that only selective information is passed to the next state. How many if you get this intuition? Okay. So, that is why any time you introduce parameters you have more flexibility in learning whatever you intend to learn. There is remember one clear difference here right and that is why I said that, while I was giving the analogy I was really setting up things, but here there is one distinction, what is the distinction that is there? Ideally what would I have wanted? Suppose I take the example of the review. Okay? And the review was say the movie was long, but really amazing. Okay? Now, which is the word here which is actually trying to mislead? So, overall sentiment is positive. Right? Everyone agrees with that, but which is the word which is misleading long right that means, I need to do what to that word? Forget that word right. Now ideally, I would have wanted someone telling me retain, retain, retain, forget, retain, retain, retain. I would have a label for each of these words and then I could have a loss function, which tells me whether my gates were actually adhering to this decisions or not. Right? So, remember my gates are learning some distribution o t minus 1, which tells me what fraction to retain. And at this particular time step I would have wanted o t minus 1 to be all 0s. Okay? I would have wanted to forget, but this kind of info not just o t minus 1 this will become more clear when I do all the other gates also. So, what I am trying to say is that you should have had some supervision, which tells you which information to retain and which information to forget, but you do not have the supervision right. No one is telling you these are the important words these are not the important words. So, So, that is the difference between the whiteboard analogy, there you knew exactly which step is important and which step is not important, here you do not know that. All you know is that you have a final loss function which depends on plus or minus whether the this prediction is close to positive or close to negative, and what is the loss, and that loss is what is being back propagated. But the difference now is that, you have introduced a model which can learn to forget somethings right. Earlier you did not have a model which could learn to write or read or forget selectively. Now, you have introduced a model this is a better modeling choice right. So, the same as we have had arguments that you could do y is equal to w transpose x or you could do y is equal to deep neural network of x right. We are making different modeling choices here and with the hope that one modeling choice is better than the other choice. So, just as RNN was one modeling choice, now you are using a different modeling choice where again with the help of these gates and all you can definitely write a function of y is a function of the input and that function is going to be the LSTM function, which we will see in detail. So, this is one part of that function. And while doing this you are just making a better modeling choice which allows you to learn more parameters and along the way if important do selective write read and forget, is that clear right? So, you would see the difference what would have been the ideal case and what is it that you have? The ideal case would have been explicit supervision for what to forget read and write, you will never have that, but you are still making a modeling choice which allows you to do that. So, if it is required the model while back propagation should be able to learn these parameters. So, get you are able to do that. I know I am repeating myself, but it is very important that you understand this distinction. How many of you get this now? And as I said these parameters will be learned along with the other parameters and o t is called the output gate, because it decides what to output to the next cell state okay. Still you see that there is a lot of gap here, we have not reached s t yet, we are still at s t minus 1, we have computed some intermediate value, but we have not reached s t yet. And along the way we had 3 things selective write read and forget, we have only taken care of selective write so far okay. Now, let us look at selective read. So, what does selective read do? You are going to get new information at time step t which is x t right. And now instead of this original cell state, you have used the selectively written cell state, because that is what you have written now. So, that is what you should use. Now using a combination of these 2, I am going to compute some intermediate value okay. And just stare at this equation, this equation form is very similar to the RNN equation form right, only thing is that instead of s t minus 1, I am using h t minus 1 and for good reason, because I know that h t minus 1 contains only selectively written values from s t minus 1, is that fine? And x t is the new input. Still there is some gap here, I have not reached s t yet, I am still at an intermediate value. So, this is the new input which I have received. Now, what should I do with this new input? Selectively read this input, I do not want to take all of this input, because maybe the input which I have got now is a stop word, and I do not want to read all of it right. Do you get that? So, now, it captures all the information from the previous state as well as the current input, and you want to selectively read this. So, now, what would you do to selectively read? Again the same situation that you have an s tilde, the answer is already here. You have an s tilde, and you do not want to pass it on as it is to s t, this is s tilde, s t somewhere here which you do not know how to get to, but you know that you do not want to pass on all the input that you have read, you want to selectively pass it on. So, what will you do now? Again introduce a gate, and this gate will be called? Input gate or the read gate right okay. So now, what can you give me an equation for the gate? I t is equal to? Sigma of okay that is good, because sigmoid is what we will need it is going to be a fractional thing. Let me add the easy part, w into h t minus 1 that is telling you what has happened so far and u times x t. You see the same equation, same form the parameters have changed. So, these you will call as w I u I and b I and they are depending on the input as well as the previous state, previous temporary state that we had computed okay. So, that is exactly what your input gate is going to be and now this operation is the selectively reading operation. How many if you are fine at this point? Okay. And then this product is going to use to be is will help us to read selectively from this temporary value that we have constructed or the input that we have taken okay. So, so far what do we have? We have the following, we have the previous state which was s t minus 1, then we have an output gate which was o t minus 1, using these 2 we have done selective right right. We have taken the previous state and the gate and then a selective right, is that fine okay. And we need to check if the sigmoid should come here, because sigmoid is already there in the computation of s t minus 1 right. It is not there. So, this already has 1 sigmoid right, yeah. So, then again a sigmoid on that is it there, okay. We will figure it out just check the equation right. So, there may or may not be the sigmoid, the sigmoid might already applied to s t minus 1, but we can figure that out okay. So, this is the selective right portion, Then you compute the current temporary state okay, and just look at the similarity between these equations. Then you have an input gate, and using these 2 you have done a selective read okay. So, I have taken care of selective write and selective read, but you are still not reached s t, I still do not have an arrow here, I still need to figure out how to compute the s t finally, okay. So, what is the operation which is remaining now selective forget.Okay. So, what do you think should we forget? Do you want to find new s t? So, let us see what we will forget right. So, the question now is that, you had this s t minus 1, and now you have a temporary state s tilde t, which is here, how do we combine these 2 to get the new cell state okay. So, the simplest way would be, that you say that s t is equal to whatever was there in s t minus 1, plus selectively reading from the current input, is that fine? This is one way of doing it okay. But now what am I doing here, what is the problem here? I am reading, I am taking s t minus 1 as it is right. So, what should I do? I should forget some parts of s t minus 1. So, what should I do for that? Introduce a what gate? Forget gate right. So, we may not want to use s t minus 1 as it is, but we want to forget. So, there is at this point all of you should get some confusion, if you do not then I would be worried, if you are getting some confusion good right, you should all get confused at this point. Why are you confused? Because you already read selective right, and now again you are doing a selective forget also right right. But there is a difference, because the selective right was then used to compute how to read the information right. But now once you have read the new information, you want to see how to assimilate it back with the old information that you had right. So, that is why you introduce a separate gate. So, think of it as this way that, you start keeping these functions separate input, output and forget. So, they can separately learn things okay. So, whatever you want to selective right, let it be a separate function. These h t minus 1 is not going back to s t right. Let us just be used so that you can compute these temporary states. So, that is what is being passed to the next temporary state. Let I t only decide how much of this input should be read okay. And then when you want to combine these 2, just use a separate gate. And this exact idea which is confusing all of you, why have a separate right gate and a separate forget gate, led to something known as gated recurrent units, where they merged these 2 gates okay. So, we will get back to that okay. So, at this point it is fine, I am just telling you the original equations for LSTM and this was the motivation that they had. So, as I said there are at least 15 to 20 different variants of LSTM, which use different equations they tie some of these weights. So, one thing could be that forget is the same as 1 minuteus remember right or output could be same as 1 minuteus input right. So, you could have tied these gates instead of learning separate parameters for them. So, in the most parameterized form you have a separate parameter for all of these okay. So, we introduce the forget gate, again can you tell me a form for this forget gate, f t is equal to first term, w f, second term u f, what would be there in the second term? X t and the first term okay. So, this is what it will look like okay. So, if you remember one of these equations you will be able to write all of these, not that I am going to ask you to write them in a quiz or something, but why take a chance. So, and then once you have computed the forget gate, instead of this equation can you tell me what is the equation that you are going to use? What is the first time going to be? It is s t minus 1 here, what is it going to be now? F t into? S t minus 1, is it fine, okay. So now, we have the full set of equations for LSTM, we have certain gates and certain states. What are the gates? Output gate, input gate, forget gate. Why do you guys have this momentary amnesia like suddenly forget everything? Okay. So, output gate, input gate and forget gate, all of these have the same form with different parameters okay. What about the states, which are the states that we have computed one was s t, the other was h t and the third one was s tilde t okay, s tilde t, from s tilde t we get s t and from s t we compute h t okay. So, yeah. So, in the diagram that you see here at the top, tell me which are the computations which are happening at one time step, at time step t, which are the computations which are happening, is it I will give you the options right, is it this or is it this okay, let us call this 1, let us call this 2 or this 3 or this 4, which are the computations happening at one time step? And you see the order also here, this should be straightforward right, why? How many of you say 4? That is the one right, because you start with selective reading right, and you can just go by these right, these are all index by t right, is that fine okay. So, these are the computations would happen at time step t, and these are exactly the computations which were written right. So, we have the 3 gates, which you need to compute at every time step, and you have the 3 states which you need to compute at every time step, is that fine. And this s t minus 1 is not being computed it is just taken from the previous time step, is that okay fine. So, you have these 6 computations would happen at every time step, and the output final output of an LSTM. So, when you use tensorflow or something, the output of an LSTM would give you two things it will give you h t comma s t okay. Well these are both the states that are being computed, one is the running state and one is the current state which is being computed okay. And I chose the notation s because that is what we have been using for RNNs, but in LSTM and all the literature instead of s you will find it to be c t okay, because it is called the cell state so that is why c t okay. So, all these equations wherever you see an s s, when you are reading some standard blogs or things like that you will see c instead of s right, you just do this mapping in your head okay. So, LSTM actually has many variants, which include different number of gates and also different arrangement of the gates. So, as I was saying that you could say that input is 1 minus output or input is 1 minus forget or things like that. And also why this particular parametric form. Right? Why not make w naught into s t minus 1 instead of h t minus 1 and so on. So, all kinds of things that you could do and all of these are valid, these are all valid variance of LSTMs. So, there is this paper called LSTM or search space ODC. So, you can go and look at I think we link it in the pre in the reading material right. So, you can see that there are actually many, many variants of LSTMs, but this is the more standard and default variant which will find in most platforms on TensorFlow or PyTorch and so on. And there is another very popular variant of LSTMs which is called gated recurrent units. So, we will just see gated recurrent units. So, I will just give you the full set of equations for GRUs. So, you have gates, but unlike LSTMs you have only 2 gates, you have an output gate and you have an input gate, you do not have the forget gate okay. So, what am I going to do for the forget gate? So, this is what I am going to do, you see the last equation. So, instead of forget gate, I am just saying that okay, this is what you are going to selectively input from the current temporary state. So, the rest of you rest of it you take from the previous state right. So, I have just tied the input gate and the forget gate okay. Any other changes do you see in this? So, earlier we had h t minus 1 everywhere right. Now, we have s t minus 1 itself is that fine okay. So, the basic idea right these equations are many many and you could think of your own equations, you could say that I will not really use this input information at all or I will choose to use it differently or what not right. There are several things that you could do, at a very abstract level this is what you need to understand what is this here. So, these parameters could then make a difference right, they could adjust it accordingly and so on right. So, that is what I was coming. So, the there are various ways of realizing this right, at the abstract level you need to understand that the original problem was trying to store all the information from time step 1 to time step t, capital T right, which is not feasible because of this finite size that you have. So, along the way we build this intuition that it should be good to have these operations which allow you to selectively read write and forget right. How do you mathematically realize these operations? There are various, various choices for doing that and we saw a few choices for doing that right. There are many others you could have done, but this is largely what whenever you say that I am using LSTM, most likely you are using the set of equations which I saw, which we saw on the previous slide and whenever you are using a GRU these are the set of equations that you will be using okay. And again remember this that there is no explicit supervision here, it is just that we have a better modeling choice, we are just introduce more parameters. So, that if required these parameters could be adjusted to do a selectively read write and forget right. So, it is often it is often valuable, if you are doing some task with RNNs or LSTMs, you should visualize these gates right, you should see that a time step t, if you thought that it should have forgotten everything that it hasSo far, because suppose you had this the movie was long, but I really loved it, because the direction was superb and so on. Now, this word but actually changes everything right, because it whatever was written before it does not matter anymore right. So, is it really learning those kind of gates where everything before, but was forgotten right. So, it would be helpful to visualize these output gates and see what kind of values they are learning, what what kind of things they are remembering, forgettable, forgetting and selectively reading and so on right. So, as I said I will just again summarize the key thing here is the intuition and then the realization in the form of equations there are multiple choices, we have seen a few of those, right that is what I will end with. And in particular in GRUs there is no explicit forget gate and instead of s t minus 1 you use s t minus 1 everywhere.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sppech_tp_text('/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fine. So, with that motivation let us go to the next module, where we will talk about long short term memory and gated recurrent units okay. So, now, all this is fine in terms of okay, I gave you a derivation on the board and say that this is not required, but can I give you a concrete example where RNNs also need to selectively read write and forget right? Only then you will be convinced that this kind of morphing is bad in the case of RNNs. So, I will start with that example. And then once we agree that we need selective read write and forget, how do we convert this into some mathematical equations, right? Because conceptually it is fine, but you have to write some equations so that the R n can do some computations where you have selective read write and forget, right? So, that is what we are going to do over the rest of the lecture. So, first let me start with a concrete example, where you want to predict the sentiment of a review using an RNN. So, this is the RNN structure, we have done this in the past that you have a sentence, one word at a time is your every time step, you will feed this to the RNN, and at the last time step you will make a prediction okay. And as I said the RNN reads the document from left to right, and by the time it reaches the end the information obtained from the first few words is completely lost right. Because it is a long document and you continuously writing to the same cell state. So, I will use the information that you had gained at the previous time step, but ideally we want to do the following. We want to forget the information added by stop words, like a and these do not contribute to the sentiment of it. I can ignore these words and still figure out the sentiment of the document. I want to selectively read the information added by previous sentiment bearing words. So, when I have reached the last time step, I should be able to read everything else which had some sentiments before it and focus on those. Just I want to selectively read from these sentiment bearing words, and also I want to selectively write the new information. So, I have read the word performance now, I want to selectively write it to the memory whether I should write it completely or should I only write parts of it or not that is what I need to decide right. So, that is fair this is a typical example where RNN also when it is reading with long documents, it needs to understand what is the important information in the document that needs to be retained and then selectively read right and forget okay. So, I am spending a lot of time on this analogy because you need to really understand that this is important and this is where RNN suffer right. If you are using them for very very long documents, if you have documents of the size 1,000 words, which is not common, which is not uncommon right? Because Wikipedia pages have much more than that per document. So, it is going to be very hard to encode the entire document using an RNN. Not that it is going to become significantly easier with LSTMs or GRUs, but to certain extent it will become easier okay. Now, the next part is how do we convert this intuition into some mathematical equations right. So, let us look at that. So, in this diagram recall that the blue colored vector is called the state of the RNN, it has a finite size. So, now I will just call it as s t belongs to some RNN, and the state is analogous to the whiteboard and sooner or later it will get overloaded with information and we need to take care of this right. So, now, our wish list is selectively read write and forget okay. So, let us start with that. So, what we want to do is that, and this is the problem definition now, that we have computed the state of the RNN, this this is the blue colored vector although it is not blue, but this is the blue colored vector from the previous diagram, where the state of the RNN was computed. I know what the state is at time step s t minus 1. Now, I want from here to here go from here to here that means, from s t minus 1, I want to compute the new state of the, RNN, right. So, I had something written on the whiteboard, I want to write something new, I want to change the state of the whiteboard and this is the new information that has coming to me, right, x t is the new information at time step t. And while doing this I want to make sure that I use selectively write, read and forget. So, these three operations have to come in somewhere in the between. So, that I am true or faithful to the analogy which I have been giving right that is the question. This is our problem definition now, going from s t minus 1 to s t and introducing these 3 operations along the way okay that is what we are interested in doing. And we will go 1 by 1 we will implement each of these 3 items right. So, we will start with selective right. So, recall that in RNNs this is what happens at every time step t, you take the previous time step, previous l state, you take the current input, do you recognize the operation here? How many of you recognize the operation here? Use your hands okay. So, this is nothing, but the following operation and as usual I have ignored the bias okay, is that fine? So, that is what I am representing it as. But now so, one way of looking at it is that, when I am computing s t, I am reading or I am taking the whole of s t minus 1. So, once I have computed s t minus 1, I am writing this to my whiteboard, and then whole of it would be used to compute the next time step okay. But instead of doing this what I want is, I want to read only selective portions of s t minus 1 or rather I want to write only selective portions of s t minus 1. Once I have computed s t minus 1, I do not want to write the whole of it, because then the whole of it will be used to compute the next cell state, I do not want that. I just want to selectively write some portions of it okay. Now, in the strictest case, since I know that s t minus 1 belongs to R n, it is an n dimensional vector. In a strictest case what I could have done is, I could have used a binary decision, that of all these n entries, I am going to read some entries and ignore the others. So, all the other entries I am going to set to 0 fine, that is the strictest thing that you could have done. Now for any of these strictest things what is the soft solution? So, for binary what's the soft solution binary 0 to 1. So, what's the soft solution for that between 0 to 1, to reach some fraction of each of these dimensions. Right? So, let's try to understand what I'm trying to do here. Okay? So, on the 3rd bullet some of these entries should have gone to 0. Right? Okay? So, instead of doing this what we want to do is, we have this vector which has n entries, this is the cell state at t minus 1. Now, I do not want to write the entire vector onto the final cell state, what I want to do is, I will take some fractions of it say 0.2 of this, 0.3 of this, 0.4 of these and then write only that. Do you see the operation that I am trying to do Right? I want to take some fractions and write only those to the set. Okay? And as I said this is a softer version of the hard decision, which would have been 0 for this, 1 for this, again 0 for this and so on right. How to do this, why to do this all that is not clear, I am just telling you the intuition how and why will become clear later, is that fine okay. So, we want to be able to take s t minus 1 and write only selective portions of it or pass only selective portions of it to s t. So, whenever we compute s t, we do not want to write the whole of s t minus 1, just want to use selective portions of that. So, what we do is, we introduce something known as a gate okay. And so, this gate is o t minus 1 okay. We take the original cell state s t minus 1, do an element wise product with a gate which is known as the output gate, and then write that product to a new vector which is h t minus 1 okay. So, initially this will look confusing, but it will become clear by the end of this lecture okay. So, is that fine? This is what I am trying to do. Again how to do this is not clear, but this still matches the intuition which I have been trying to build that I want to write only selective portions of the data which I already have, is that fine okay. So, each element of o t minus 1 gets multiplied by the corresponding element of s t minus 1, and it decides what fraction is going to be copied and this o t minus 1 is going to be between 0 to 1. But how do I compute o t minus 1? How does the RNN know what fraction of the cell state to get to the next state? How will it do it? We need to learn something, whenever you want to learn something what do we introduce? Everyone parameter, sorry. What did you guys say? Back propagation, Back propagation will do what? It will work in the air or propagate to what? Whenever you want to do some kind of a learning, I want to learn some function, what do I introduce? Parameter. Parameter right. So, that is what we are going to do. We are now going to introduce a parametric form for o t minus 1 right. And remember this throughout in machine learning whenever you want to learn something always introduce a parametric form of that quantity, and then learn the parameters of that function. Do you get this, how many if you get this statement? Okay. This is what we have been saying day from right from class 2 or class 3 right. Always introduce a parametric function for your input and output and learn the parameters of this function. So, that is exactly what I am going to do, I am going to say that o t minus 1 is actually this function, I am just giving you some time to digest this. So, this is a time step t minus 1. So, it depends on the input at time step t minus 1. It also depends on the output at output means whatever comes out of this right. So, the same operation would have happened at time step t minus 2. So, whatever was the output at that state it will also depend on this. So, just take a while to digest this equation, you will see at least 6 more equations of this form in this lecture. So, if you are comfortable with 1 all of them would be clear. So, try to connect the whole story, I have s t minus 1, I do not want to pass it as on or pass it on as it is.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.results.channels[0].alternatives[0].transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m audio_file\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m transcription \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mtranscriptions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      6\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m   file\u001b[38;5;241m=\u001b[39maudio_file\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resul = model.transcribe('/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_2.mp3')x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenpoliasetty/Downloads/David2.0/david2.0/facerecog/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fine. So, with that motivation let us go to the next module where we will talk about a long short term memory and gated recurrent units ok. So, now, all this was fine in terms of ok I gave you a derivation on the board and say that this is not required, but can I give you a concrete example where RNNs also need to selectively read write and forget right only then you will be convinced that this kind of morphing is bad in the case of RNN. So, I will start with that example and then once we agree that we need selective read write and forget how do we convert this into some mathematical equations right because conceptually it is fine, but you have to write some equations so that the RNN can do some computations where you have selective read write and forget right. So, that is what we are going to do over the rest of the lecture. So, first let me start with a concrete example where you want to predict the sentiment of a review using an RNN. So, this is the RNN structure we have done this in the past that you have a sentence one word at a time is your every time step you will feed this to the RNN and at the last time step you will make a prediction ok. And as I said the RNN reads the document from left to right and by the time it reaches the end the information obtained from the first few words is completely lost right because it is a long document and you continuously writing to the same cell states. So, you lose the information that you had gained at the previous time step, but ideally we want to do the following. We want to forget the information added by stop words like a and these do not contribute to the sentiment of I can ignore these words and still figure out the sentiment of the document. I want to selectively read the information added by previous sentiment bearing words. So, when I have reached the last time step I should be able to read everything else which had some sentiments before it and focus on those. So, I want to selectively read from these sentiment bearing words and also I want to selectively write the new information. So, I have read the word performance now I want to selectively write it to the memory whether I should write it completely or should I only write parts of it or not that is what I need to decide right. So, that is fair this is a typical example where RNN also when it is dealing with long documents it needs to understand what is the important information in the document that needs to be retained and then selectively read right and forget ok. So, I am spending a lot of time on this analogy because you need to really understand that this is important and this is where RNN suffer right. If you are using them for very very long documents if you have documents of the size 1000 words which is not common or which is not uncommon right because Wikipedia pages have much more than that per document. So, it is going to be very hard to encode the entire document using an RNN not that it is going to become significantly easier with LSTMs or GRUs, but to certain extent it will become easier ok. Now, the next part is how do we convert this intuition into some mathematical equations right. So, let us look at that. So, in this diagram recall that the blue color vector is called the state of the RNN it has a finite size. So, now, I will just call it as ST belongs to some RNN and the state is analogous to the whiteboard and sooner or later it will get overloaded with information and we need to take care of this right. So, now, our wish list is selectively read write and forget ok. So, let us start with that. So, what we want to do is that and this is the problem definition now that we have computed the state of the RNN this is the blue colored vector although it is not blue, but this is the blue colored vector from the previous diagram where the state of the RNN was computed I know what the state is at time step ST minus 1. Now, I want from here to here go from here to here that means, from ST minus 1 I want to compute the new state of the RNN right. So, I had something written on the whiteboard I want to write something new I want to change the state of the whiteboard and this is the new information that has coming to me right X t is the new information at time step t and while doing this I want to make sure that I use selectively write read and forget. So, these three operations have to come in somewhere in the between. So, that I am true or faithful to the analogy which I have been giving right that is the question this is the problem definition now going from ST minus 1 to ST and introducing these three operations along the way ok that is what we are interested in doing and we will go one by one we will implement each of these three items right. So, we will start with selective write. So, recall that in RNNs this is what happens at every time step t you take the previous time step previous L state you take the current input do you recognize the operation here how many if you recognize the operation here these are hands ok. So, this is nothing, but the following operation and as usual I have ignored the bias is that fine. So, that is what I am representing it as, but now. So, one way of looking at it is that when I am computing ST I am reading or I am taking the whole of ST minus 1. So, once I have computed ST minus 1 I am writing this to my whiteboard and then whole of it would be used to compute the next time step ok, but instead of doing this what I want is I want to read only selective portions of ST minus 1 or rather I want to write only selective portions of ST minus 1. Once I have computed ST minus 1 I do not want to write the whole of it because then the whole of it will be used to compute the next cell state I do not want that I just want to selectively write some portions of it ok. Now, in the strictest case since I know that ST minus 1 belongs to RN it is an n dimensional vector in a strictest case what I could have done is I could have used a binary decision that of all these n entries I am going to read some entries and ignore the others. So, all the other entries I am going to set to 0 fine that is the strictest thing that you could have done. Now, for any of these strictest things what is the soft solution? So, for binary what is the soft solution binary 0 to 1. So, what is the soft solution for that between 0 to 1. So, read some fraction of each of these dimensions right. So, let us try to understand what I am trying to do here ok. So, on the third bullet some of these entries should have gone to 0 right ok. So, instead of doing this what we want to do is we have this vector which has n entries this is the cell state at t minus 1. Now, I do not want to write the entire vector onto the final cell state what I want to do is I will take some fractions of it say 0.2 of this, 0.3 of this, 0.4 of these and then write only that you see the operation that I am trying to do right. I want to take some fractions and write only those to the cell ok. And as I said this is a softer version of the hard decision which would have been 0 for this, 1 for this, again 0 for this and so on right. How to do this, why to do this all that is not clear I am just telling you the intuition how and why will become clear later is that fine ok. So, we want to be able to take s t minus 1 and write only selective portions of it or pass only selective portions of it to s t. So, whenever we compute s t we do not want to write the whole of s t minus 1 just want to use selective portions of that. So, what we do is we introduce something known as a gate ok and so, this gate is o t minus 1 ok. We take the original cell state s t minus 1 do an element wise product with a gate which is known as the output gate and then write that product to a new vector which is h t minus 1 ok. So, initially this will look confusing, but it will become clear by the end of this lecture ok. So, is that fine this is what I am trying to do again how to do this is not clear, but this still matches the intuition which I have been trying to build that I want to write only selective portions of the data which I already have is that fine ok. So, each element of o t minus 1 gets multiplied by the corresponding element of s t minus 1 and it decides what fraction is going to be copied and this o t minus 1 is going to be between 0 to 1, but how do I compute o t minus 1 how does the R in and no what fraction of the cell state to get to the next state how will it do it. We need to learn something whenever you want to learn something what do we introduce everyone. Parameter. Parameter. Parameter. Parameter. Sorry what did you guys say back propagation back propagation will do what it will work in the air or. Propagate. Propagate to what. Whenever you want to do some kind of a learning I want to learn some function what do I introduce. Parameter. Parameter right. So, that is what we are going to do we are now going to introduce a parametric form for o t minus 1 right and remember this throughout in machine learning whenever you want to learn something always introduce a parametric form of that quantity and then learn the parameters of that function you get this how many if you get this statement ok this is what we have been saying they from right from class 2 or class 3 right always introduce a parametric function for your input and output and learn the parameters of this function. So, that is exactly what I am going to do I am going to say that o t minus 1 is actually this function I am just giving you some time to digest this. So, this is a time step t minus 1. So, it depends on the input at time step t minus 1 it also depends on the output at output means whatever comes out of this right. So, the same operation would have happened at time step t minus 2. So, whatever was the output at that state it will also depend on this. So, just take a while to digest this equation you will see at least 6 more equations of this form in this lecture. So, if you are comfortable with one all of them would be clear. So, try to connect the whole story I have st minus 1 I do not want to pass it as on a pass it on as it is.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Check if CUDA or MPS (Apple Silicon GPU) is available, otherwise fallback to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = whisper.load_model(\"small\")  # Whisper models are loaded on the appropriate device internally\n",
    "\n",
    "# Transcribe the audio file\n",
    "result = model.transcribe(\"/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3\")\n",
    "\n",
    "# Print the transcription\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We might try these for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq(api_key='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')\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename = '/Users/naveenpoliasetty/Downloads/David2.0/david2.0/audio_directory/audio_split_1.mp3'# Replace with your audio file!\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()), # Required audio file\n",
    "      model=\"distil-whisper-large-v3-en\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      language=\"en\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    # Print the transcription text\n",
    "    print(transcription.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facerecog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
